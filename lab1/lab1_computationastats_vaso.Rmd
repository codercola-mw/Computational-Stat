---
title: "Lab1 Computational Statistics"
author: "Vasileia Kampouraki"
date: "24/1/2020"
output: pdf_document
---

# Question 1: Be careful when comparing

*1. Check the results of the snippets. Comment what is going on.*

```{r}
x1<-1/3 
x2<-1/4

if ( x1-x2==1/12) {
print(" Subtraction is correct" )
} else {
print("Subtraction is wrong" )
}

```

```{r}

x1<-1 
x2<-1/2

if ( x1-x2==1/2){
print ( "Subtraction is correct" )
} else{
print ( "Subtraction is wrong" )
}
```

The first example gives that subtraction is wrong while we would expect it to be correct. This happens because rational numbers are rounded in R towards the closest computer float and thus the subtraction is not entirely precise us we have loss of significant digits(underflow). This proccess of rounding is called floating point arithmetic.
More precisely, R uses binary system for operations and only the rational numbers that their denominators have numbers in the power of two have a finite representation and give precise results. 

However, the second example gives the expected answer and that's because 1/2 has a finite representation and no rounding is used.


*2. If there are any problems, suggest improvements.*

```{r}
x1<-round(1/3,2) 
x2<-round(1/4,2)

if ( round(x1-x2,2)==round(1/12,2)) {
print(" Subtraction is correct" )
} else {
print("Subtraction is wrong" )
}
```

A solution as seen above is to round the rational numbers to two decimal places.


# Question 2: Derivative


*1. Write your own R function to calculate the derivative of f(x) = x in this way with $\epsilon = 10^{-15}$.*

```{r echo=F}
derivative <- function(f,e){
  der <- ((f + e) - f)/e
  return(der)
}

```

*2. Evaluate your derivative function at x = 1 and x = 100000.*

```{r}
derivative(1,10^-15)
derivative(100000,10^-15)
```

*3. What values did you obtain? What are the true values? Explain the reasons behind the discovered differences.*

We obtain 1.110223 for x=1 and 0 for x=100000 when we would expect 1 in both cases.


# Question 3: Variance


### 1)
```{r}

myvar <- function(x){
  n <- length(x)
  var <- 1/(n-1)*(sum(x^2)- 1/n*(sum(x))^2)
  return(var)
}
  
```

### 2)

```{r}
set.seed(12345)

x <- rnorm(10000,10^8,1)

```

```{r}
#used Mecon's code but changed i

Y <- vector(length=length(x))

X=0
for( i in (2:length(x)) ){   #start drom 2 to have variance, otherwise NaN
  X = x[1:i]
  Y[i]=myvar(X)-var(X)
}


```

 
```{r}
## plot of Variance

i <- seq(1,length(x),1)

library(ggplot2)

df <- data.frame(i=i,var=Y)

ggplot(data=df,aes(x=i,y=var)) + geom_point(colour="red") +
  xlab("i") +
 ylab("Variance")+
  ggtitle("Dependece of Yi on i")


```


We would expect $y_{i}$ to be 0 for each i as our function for computing the variance theoritically would give the same results as the built-in function var().However this doesn't happen as we see from the results.
For example, myvar(X[1:4])= 0 whereas var(X[1:4])=0.309922 so we see that we obtain different results from those fuctions.
Again, those differences occur because of the floating point system used in R where the numbers get rounded and that's why we don't get precise results.(GIVE A BETTER EXPLANATION)
Looking at he Plot of $Y_{i}$ against i we see a not normal image of the variance because as we said the variance ranges between  -5.089959 and 4.407091
(those values where acquired using Y[which.min(Y)] and [which.max(Y)] commands,respectively) while it should be 0.

# Question 4: Linear Algebra

*The Excel file "tecator.xls" contains the results of a study aimed to investigate whether a near infrared absorbance spectrum and the levels of moisture and fat can be used to predict the protein content of samples of meat. For each meat sample the data consists of a 100 channel spectrum of absorbance records and the levels of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The moisture, fat and protein are determined by analytic chemistry. The worksheet you need to use is "data" (or file "tecator.csv"). It contains data from 215 samples of finely chopped meat. The aim is to fit a linear regression model that could predict protein content as function of all other variables.*


### 1. Import the data set to R

```{r}
library(readxl)
data  <- read_excel("tecator.xls")
```


### 2. Optimal regression coefficients can be found by solving a system of the type $A\vec{\beta} =\vec{b}$ where $A = X^{T}X$ and $\vec{b} = X^{T}\vec{y}$. Compute A and $\vec{b}$ for the given data set. The matrix X are the observations of the absorbance records, levels of moisture and fat, while $\vec{y}$ are the protein levels.


```{r echo=F}
X <- data[-c(1,103)] #observations
X <- as.matrix(X)
y <- data[103]    #target
y <- as.matrix(y)

A <- t(X)%*%X
b <- t(X)%*%y
```


### 3. Try to solve $A\vec{\beta} =\vec{b}$ with default solver solve(). What kind of result did you get? How can this result be explained?

```{r echo=F}
#beta <- solve(A)%*%b
```

We get the error "system is computationally singular: reciprocal condition number = 7.13971e-17" .
This error actually tells us that A matrix is singular ; i.e. not inverse.
The reciprocal condition number is $\frac{1}{\kappa}$ where $$\kappa$$ is the condition number.

Generally the condition number has to do with how much the output will change if the input changes and is defined as:
   $\kappa(A) = \left\|A^{-1}\right\| \left\|A\right\| $

In our case, condition number is an indication of multicollinearity in the A matrix.

### 4. Check the condition number of the matrix A (function kappa()) and consider how it is related to your conclusion in step 3.

```{r}
kappa(A)
```

As we can se the condition number $$\kappa(A)$$ is a big number and verifies what we said in the previous question.

### 5. Scale the data set and repeat steps 2-4. How has the result changed and why?

```{r}
data1 <- as.data.frame(scale(data))

Xnew <- data1[-c(1,103)] #observations
Xnew <- as.matrix(Xnew)
ynew <- data1[103]    #target
ynew <- as.matrix(ynew)

A1 <- t(Xnew)%*%Xnew
b1 <- t(Xnew)%*%ynew


beta1 <- solve(A1)%*%b1

condnum <- kappa(A1)
condnum
```

Now we observe that $A\vec{\beta} =\vec{b}$ can be computed and also we have smaller conditional number ($\kappa(A1)=490471520662$).
As we mentioned previously, the big condition number was an indication of multicollinearity ; e.i. strongly correlated variables.
However, by scaling the date we eliminate the strong correlations between the variables and matrix A becomes invertible.

```{r}
options(scipen=999)
```

# Appendix

```{r ref.label=knitr::all_labels(), echo=T, eval=F}

```
